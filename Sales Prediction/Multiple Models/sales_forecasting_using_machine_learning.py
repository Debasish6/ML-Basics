# -*- coding: utf-8 -*-
"""Sales Forecasting using Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A6Md61zbLtfPwzsjEue2vL395nbfgYjj

## Importing Important Pakages
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import matplotlib.colors as col
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import datetime
from pathlib import Path
import random

# Scikit-Learn models:

from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from xgboost.sklearn import XGBRegressor
from sklearn.model_selection import KFold, cross_val_score, train_test_split

# LSTM:

import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.callbacks import EarlyStopping
# from keras.utils import np_utils
from tensorflow.keras.utils import to_categorical   #Use it instead of np_utils
from keras.layers import LSTM

# ARIMA Model:

import statsmodels.tsa.api as smt
import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse


import pickle
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)

"""## Loading Dataset"""

dataset = pd.read_csv('/content/sample_submission.csv')

df = dataset.copy()

df.head()

df.tail()

df.info()

"""#### Loading Sales Data for training"""

sales_data = pd.read_csv('/content/train.csv')

df_s = sales_data.copy()

df_s.head()

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(_df_19['date'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(_df_19, x='sales', y='date', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

from matplotlib import pyplot as plt
_df_18['sales'].plot(kind='line', figsize=(8, 4), title='sales')
plt.gca().spines[['top', 'right']].set_visible(False)

from matplotlib import pyplot as plt
import seaborn as sns
_df_13.groupby('date').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
_df_12['sales'].plot(kind='hist', bins=20, title='sales')
plt.gca().spines[['top', 'right',]].set_visible(False)

df_s.tail()

df_s.info()

df_s.describe()

df_s['sales'].describe()

#Ploting using plot() function
df_s['sales'].plot()

# Monthly Or Yearly Sales function
def monthlyORyears_sales(data,time=['monthly','years']):
    data = data.copy()
    if time == "monthly":
        # Drop the day indicator from the date column:
        data.date = data.date.apply(lambda x: str(x)[:-3])
    else:
        data.date = data.date.apply(lambda x: str(x)[:4])

   # Sum sales per month:
    data = data.groupby('date')['sales'].sum().reset_index()
    data.date = pd.to_datetime(data.date)

    return data

#Creating Monthly sales data
df_s_m = monthlyORyears_sales(df_s,'monthly')

df_s_m['sales'].plot()

df_s_m['sales'].describe()

df_s_m.to_csv('/content/monthly_sales.csv')

df_s_m.head()

#Creating yearly sales data
df_s_y = monthlyORyears_sales(df_s,'yearly')

df_s_y.head()

df_s_y['sales'].describe()

df_s_y['sales'].plot()

layout = (1, 2)

raw = plt.subplot2grid(layout, (0 ,0))
law = plt.subplot2grid(layout, (0 ,1))

years = df_s_y['sales'].plot(kind = "bar",color = 'mediumblue', label="Sales",ax=raw, figsize=(12,5))
months = df_s_m['sales'].plot(marker = 'o',color = 'darkorange', label="Sales", ax=law)

years.set(xlabel = "Years",title = "Distribution of Sales Per Year")
months.set(xlabel = "Months", title = "Distribution of Sales Per Mounth")

sns.despine()
plt.tight_layout()

years.legend()
months.legend()

"""# Data Preprocessing

### ARIMA Modeling
"""

def sales_time(data):
    """Time interval of dataset:"""

    data.date = pd.to_datetime(data.date)
    n_of_days = data.date.max() - data.date.min()
    n_of_years = int(n_of_days.days / 365)

    print(f"Days: {n_of_days.days}\nYears: {n_of_years}\nMonth: {12 * n_of_years}")

sales_time(df_s)

# Sales Per Store

def sales_per_store(data):
    sales_by_store = data.groupby('store')['sales'].sum().reset_index()

    fig, ax = plt.subplots(figsize=(8,6))
    sns.barplot(x='store',y='sales',data= sales_by_store, color='darkred')

    ax.set(xlabel = "Store Id", ylabel = "Sum of Sales", title = "Total Sales Per Store")

    return sales_by_store

sales_per_store(df_s)

# Mean Monthly Sales

average_m_sales = df_s_m.sales.mean()
print(f"Overall Avarage Monthly Sales: ${average_m_sales}")

def avarage_12months():
# Last 1 years (this will be the forecasted sales):
    average_m_sales_1y = df_s_m.sales[-12:].mean()
    print(f"Last 12 months average monthly sales: ${average_m_sales_1y}")
avarage_12months()

"""# Determining Time Series Stationary"""

def time_plot(data, x_col, y_col, title):
    fig, ax = plt.subplots(figsize = (15,8))
    sns.lineplot(x=x_col, y=y_col, data = data, ax = ax, color = 'darkblue', label='Total Sales')

    s_mean = data.groupby(data.date.dt.year)[y_col].mean().reset_index()
    s_mean.date = pd.to_datetime(s_mean.date, format='%Y')
    sns.lineplot(x=(s_mean.date + datetime.timedelta(6*365/12)), y=y_col, data=s_mean, ax=ax, color='red', label='Mean Sales')

    ax.set(xlabel = "Years",
           ylabel = "Sales",
           title = title)

time_plot(df_s_m, 'date', 'sales', 'Monthly Sales Before Diff Transformation' )

def get_diff(data):
    """Calculate the difference in sales month over month:"""

    data['sales_diff'] = data.sales.diff()
    data = data.dropna()

    data.to_csv('./stationary_df.csv')

    return data

stationary_df = get_diff(df_s_m)
time_plot(stationary_df, 'date', 'sales_diff',
          'Monthly Sales After Diff Transformation')

"""# Preparing Dataset Modeling"""

def build_arima_data(data):
    """Generates a csv-file with a datetime index and a dependent sales column for ARIMA modeling."""

    da_data = data.set_index('date').drop('sales', axis=1)
    da_data.dropna(axis=0)

    da_data.to_csv('./arima_df.csv')

    return da_data

# Creating ARIMA Data Frame
datatime_df = build_arima_data(stationary_df)
datatime_df # ARIMA Dataframe

# Observing Lags
def plots_lag(data, lags=None):
    """Convert dataframe to datetime index"""
    dt_data = data.set_index('date').drop('sales', axis=1)
    dt_data.dropna(axis=0)


    law  = plt.subplot(122)
    acf  = plt.subplot(221)
    pacf = plt.subplot(223)

    dt_data.plot(ax=law, figsize=(10, 5), color='orange')
    # Plot the autocorrelation function:
    smt.graphics.plot_acf(dt_data, lags=lags, ax=acf, color='mediumblue')
    smt.graphics.plot_pacf(dt_data, lags=lags, ax=pacf, color='mediumblue')

    # Will also adjust spacing between subplots to minimize the overlaps:
    plt.tight_layout()

plots_lag(stationary_df, lags=24);

"""## Regressive Modeling"""

#create a data frame for transformation from time series to supervised:

def built_supervised(data):
    supervised_df = data.copy()

    # Create column for each lag:
    for i in range(1, 13):
        col_name = 'lag_' + str(i)
        supervised_df[col_name] = supervised_df['sales_diff'].shift(i)

    # Drop null values:
    supervised_df = supervised_df.dropna().reset_index(drop=True)

    supervised_df.to_csv('./model_df.csv', index=False)

    return supervised_df

model_df = built_supervised(stationary_df)
model_df

model_df.info()

"""## Functions For Modeling"""

# Train Test Split --- We detach our data so that the last 12 months are part of the test set and the rest of the data is used to train our model.
def train_test_split(data):
    data = data.drop(['sales','date'], axis=1)
    train , test = data[:-12].values, data[-12:].values

    return train, test

train, test = train_test_split(model_df)
print(f"Shape of  Train: {train.shape}\nShape of  Test: {test.shape}")

# Scale The Data ---- Using a min-max scaler, we will scale the data so that all of our variables fall within the range of -1 to 1.
def scale_data(train_set,test_set):
    """Scales data using MinMaxScaler and separates data into X_train, y_train,
    X_test, and y_test."""

    # Apply Min Max Scaler:
    scaler = MinMaxScaler(feature_range=(-1, 1))
    scaler = scaler.fit(train_set)

    # Reshape training set:
    train_set = train_set.reshape(train_set.shape[0],
                                  train_set.shape[1])
    train_set_scaled = scaler.transform(train_set)

    # Reshape test set:
    test_set = test_set.reshape(test_set.shape[0],
                                test_set.shape[1])
    test_set_scaled = scaler.transform(test_set)

    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1].ravel() # returns the array, flattened!
    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1].ravel()

    return X_train, y_train, X_test, y_test, scaler


X_train, y_train, X_test, y_test, scaler_object = scale_data(train, test)
print(f"Shape of X Train: {X_train.shape}\nShape of y Train: {y_train.shape}\nShape of X Test: {X_test.shape}\nShape of y Test: {y_test.shape}")

# Reverse Scaling ---- After running our models, we will use this helper function to reverse the scaling of step 2.
def re_scaling(y_pred, x_test, scaler_obj, lstm=False):
    """For visualizing and comparing results, undoes the scaling effect on predictions."""
   # y_pred: model predictions
   # x_test: features from the test set used for predictions
   # scaler_obj: the scaler objects used for min-max scaling
   # lstm: indicate if the model run is the lstm. If True, additional transformation occurs

    # Reshape y_pred:
    y_pred = y_pred.reshape(y_pred.shape[0],
                            1,
                            1)

    if not lstm:
        x_test = x_test.reshape(x_test.shape[0],
                                1,
                                x_test.shape[1])

    # Rebuild test set for inverse transform:
    pred_test_set = []
    for index in range(0, len(y_pred)):
        pred_test_set.append(np.concatenate([y_pred[index],
                                             x_test[index]],
                                             axis=1) )

    # Reshape pred_test_set:
    pred_test_set = np.array(pred_test_set)
    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0],
                                          pred_test_set.shape[2])

    # Inverse transform:
    pred_test_set_inverted = scaler_obj.inverse_transform(pred_test_set)

    return pred_test_set_inverted

# Predictions Dataframe ---- Generate a dataframe that includes the actual sales captured in our test set and the predicted results from our model so that we can quantify our success

def prediction_df(unscale_predictions, origin_df):
    """Generates a dataframe that shows the predicted sales for each month
    for plotting results."""

    # unscale_predictions: the model predictions that do not have min-max or other scaling applied
    # origin_df: the original monthly sales dataframe

    # Create dataframe that shows the predicted sales:
    result_list = []
    sales_dates = list(origin_df[-13:].date)
    act_sales = list(origin_df[-13:].sales)

    for index in range(0, len(unscale_predictions)):
        result_dict = {}
        result_dict['pred_value'] = int(unscale_predictions[index][0] + act_sales[index])
        result_dict['date'] = sales_dates[index + 1]
        result_list.append(result_dict)

    df_result = pd.DataFrame(result_list)

    return df_result

# Score The Models ---- This helper function will save the root mean squared error (RMSE) and mean absolute error (MAE) of our predictions to compare the performance of our models.

model_scores = {}

def get_scores(unscale_df, origin_df, model_name):
    """Prints the root mean squared error, mean absolute error, and r2 scores
    for each model. Saves all results in a model_scores dictionary for
    comparison."""

    rmse = np.sqrt(mean_squared_error(origin_df.sales[-12:],
                                      unscale_df.pred_value[-12:]))

    mae = mean_absolute_error(origin_df.sales[-12:],
                              unscale_df.pred_value[-12:])

    r2 = r2_score(origin_df.sales[-12:],
                  unscale_df.pred_value[-12:])

    model_scores[model_name] = [rmse, mae, r2]

    print(f"RMSE: {rmse}\nMAE: {mae}\nR2 Score: {r2}")

# Graph of Results

def plot_results(results, origin_df, model_name):
# results: a dataframe with unscaled predictions

    fig, ax = plt.subplots(figsize=(15,5))
    sns.lineplot(x=origin_df.date, y=origin_df.sales, data=origin_df, ax=ax,
                 label='Original', color='blue')
    sns.lineplot(x=results.date,y= results.pred_value, data=results, ax=ax,
                 label='Predicted', color='red')


    ax.set(xlabel = "Date",
           ylabel = "Sales",
           title = f"{model_name} Sales Forecasting Prediction")

    ax.legend(loc='best')

    filepath = Path('./model_output/{model_name}_forecasting.svg')
    filepath.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(f'./model_output/{model_name}_forecasting.svg')

def regressive_model(train_data, test_data, model, model_name):
    """Runs regressive models in SKlearn framework. First calls scale_data
    to split into X and y and scale the data. Then fits and predicts. Finally,
    predictions are unscaled, scores are printed, and results are plotted and
    saved."""

    # Split into X & y and scale data:
    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data,
                                                                 test_data)

    # Run sklearn models:
    mod = model
    mod.fit(X_train, y_train)
    predictions = mod.predict(X_test) # y_pred=predictions

    # Undo scaling to compare predictions against original data:
    origin_df = df_s_m
    unscaled = re_scaling(predictions, X_test, scaler_object) # unscaled_predictions
    unscaled_df = prediction_df(unscaled, origin_df)

    # Print scores and plot results:
    get_scores(unscaled_df, origin_df, model_name)
    plot_results(unscaled_df, origin_df, model_name)

"""## Modeling"""

# Linear Regressation
regressive_model(train, test, LinearRegression(), 'LinearRegression')

# Random Forest Regressor
regressive_model(train, test, RandomForestRegressor(n_estimators=100, max_depth=20),
          'RandomForest')

# XGBoost Regressor
regressive_model(train, test, XGBRegressor(n_estimators=100,max_depth=3,
                                           learning_rate=0.2,objective='reg:squarederror'), 'XGBoost')

# LSTM ---- LSTM is a type of recurring neural network that is especially useful for predicting sequential data. Getting started with the Keras Sequential model

def lstm_model(train_data, test_data):
    """Runs a long-short-term-memory neural net with 2 dense layers.
    Generates predictions that are then unscaled.
    Scores are printed and the results are plotted and saved."""
    # train_data: dataset used to train the model
    # test_data: dataset used to test the model


    # Split into X & y and scale data:
    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)

    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])


    # Build LSTM:
    model = Sequential()
    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]),
                   stateful=True))
    model.add(Dense(1))
    model.add(Dense(1))
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1,
              shuffle=False)
    predictions = model.predict(X_test,batch_size=1)

    # Undo scaling to compare predictions against original data:
    origin_df = df_s_m
    unscaled = re_scaling(predictions, X_test, scaler_object, lstm=True)
    unscaled_df = prediction_df(unscaled, origin_df)

    get_scores(unscaled_df, origin_df, 'LSTM')
    plot_results(unscaled_df, origin_df, 'LSTM')

from keras.models import Sequential
from keras.layers import LSTM, Dense

def lstm_model(train_data, test_data):
    """Runs a long-short-term-memory neural net with 2 dense layers.
    Generates predictions that are then unscaled.
    Scores are printed and the results are plotted and saved."""

    # Split into X & y and scale data:
    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)

    # Reshaping data for LSTM: [samples, timesteps, features]
    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

    # Build LSTM model:
    model = Sequential()

    # Define LSTM layer with corrected input shape:
    model.add(LSTM(4, input_shape=(X_train.shape[1], X_train.shape[2])))  # No batch_input_shape here.

    # Adding Dense layers:
    model.add(Dense(1))
    model.add(Dense(1))

    # Compile the model:
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

    # Fit the model:
    model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, shuffle=False)

    # Generate predictions:
    predictions = model.predict(X_test, batch_size=1)

    # Undo scaling to compare predictions against original data:
    origin_df = df_s_m
    unscaled = re_scaling(predictions, X_test, scaler_object, lstm=True)
    unscaled_df = prediction_df(unscaled, origin_df)

    # Get scores and plot the results:
    get_scores(unscaled_df, origin_df, 'LSTM')
    plot_results(unscaled_df, origin_df, 'LSTM')

lstm_model(train,test)

pickle.dump(model_scores, open( "model_scores.p", "wb" ))

"""## ARIMA Modeling"""

# SARIMAX Modeling ---- We use the statsmodels SARIMAX package to train the model and generate dynamic predictions. The SARIMA model breaks down into a few parts.

#  AR: represented as p, is the autoregressive model
#  I : represented as d, is the differencing term
#  MA: represented as q, is the moving average model
#  S: enables us to add a seasonal component

datatime_df.index = pd.to_datetime(datatime_df.index)

def sarimax_model(data):
    # Model:
    sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(12, 0, 0),
                                    seasonal_order=(0, 1, 0, 12),
                                    trend='c').fit()

    # Generate predictions:
    start, end, dynamic = 40, 100, 7
    data['pred_value'] = sar.predict(start=start, end=end, dynamic=dynamic)
    pred_df = data.pred_value[start+dynamic:end]

    data[["sales_diff","pred_value"]].plot(color=['blue', 'Red'])
    plt.legend(loc='upper left')

    model_score = {}

    rmse = np.sqrt(mean_squared_error(data.sales_diff[-12:], data.pred_value[-12:]))
    mae = mean_absolute_error(data.sales_diff[-12:], data.pred_value[-12:])
    r2 = r2_score(data.sales_diff[-12:], data.pred_value[-12:])
    model_scores['ARIMA'] = [rmse, mae, r2]

    print(f"RMSE: {rmse}\nMAE: {mae}\nR2 Score: {r2}")

    return sar, data, pred_df

sar, datatime_df, predictions = sarimax_model(datatime_df)

sar.plot_diagnostics(figsize=(12, 8));

pickle.dump(model_scores, open( "ARIMAmodel_scores.p", "wb" ))

"""## Comparing Models"""

def create_results_df():
    results_dict = pickle.load(open("model_scores.p", "rb"))

    results_dict.update(pickle.load(open("ARIMAmodel_scores.p", "rb")))

    results_df = pd.DataFrame.from_dict(results_dict, orient='index',
                                        columns=['RMSE', 'MAE','R2'])

    results_df = results_df.sort_values(by='RMSE', ascending=False).reset_index()

    results_df.to_csv('./results.csv')

    fig, ax = plt.subplots(figsize=(12, 5))
    sns.lineplot(x=np.arange(len(results_df)),y= 'RMSE', data=results_df, ax=ax,
                 label='RMSE', color='darkblue')
    sns.lineplot(x=np.arange(len(results_df)),y= 'MAE', data=results_df, ax=ax,
                 label='MAE', color='Cyan')

    plt.xticks(np.arange(len(results_df)),rotation=45)
    ax.set_xticklabels(results_df['index'])
    ax.set(xlabel = "Model",
           ylabel = "Scores",
           title = "Model Error Comparison")
    sns.despine()

    plt.savefig(f'./model_output/compare_models.png')

    return results_df

results = create_results_df()
results

avarage_12months()

average = 894478.3333333334
XGBoost = results.MAE.values[4]
percentage_off = round(XGBoost/average*100,2)

print(f"With XGBoost, prediction is within {percentage_off}% of the actual.")

results

XGBoost

